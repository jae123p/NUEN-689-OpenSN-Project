{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "580166e2-0b3d-402e-8f75-83d3281a60a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmc\n",
    "import openmc.mgxs\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas\n",
    "import h5py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02b34201-83c9-4a8e-b90e-7ba84cb93fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c7ec070-fa5b-45b3-bbb7-9ee494d2bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: OPENMC_CROSS_SECTIONS=/home/jae123p/endfb80/endfb-viii.0-hdf5/cross_sections.xml\n"
     ]
    }
   ],
   "source": [
    "os.environ['PATH'] += r':/home/jae123p/miniconda3/envs/openmc-env/bin'\n",
    "\n",
    "%env OPENMC_CROSS_SECTIONS=/home/jae123p/endfb80/endfb-viii.0-hdf5/cross_sections.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "54df814f-80fb-4b62-82fc-3badacb5b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = openmc.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "87e3cdb1-9c86-45d7-bd0d-498748563c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "oralloy = openmc.Material(name=\"Oralloy\")\n",
    "oralloy.add_nuclide('U234', 4.9210e-04)\n",
    "oralloy.add_nuclide('U235', 4.4917e-02)\n",
    "oralloy.add_nuclide('U238', 2.5993e-03)\n",
    "\n",
    "tuballoy = openmc.Material(name=\"Tuballoy\")\n",
    "tuballoy.add_nuclide('U234', 2.6299e-06)\n",
    "tuballoy.add_nuclide('U235', 3.4428e-04)\n",
    "tuballoy.add_nuclide('U238', 4.7470e-02)\n",
    "\n",
    "materials = openmc.Materials([oralloy, tuballoy])\n",
    "materials.export_to_xml('HEU_MET_FAST-003-CASE-1.xml')\n",
    "model.materials = openmc.Materials([oralloy, tuballoy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f024b1a6-a4bf-4ccc-ba18-11c3390b9bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geometry\n",
    "inner_radius = 6.7820\n",
    "outer_radius = 11.8620\n",
    "\n",
    "oralloy_sphere = openmc.Sphere(0, 0, 0, r=inner_radius)\n",
    "tuballoy_sphere = openmc.Sphere(0, 0, 0, r=outer_radius, boundary_type = 'vacuum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f706f36-82d0-43bc-9afb-499d36a97224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regions\n",
    "or_region = -oralloy_sphere\n",
    "tub_region = +oralloy_sphere & - tuballoy_sphere\n",
    "\n",
    "#Cells\n",
    "or_cell = openmc.Cell(name='or', fill = oralloy, region = or_region)\n",
    "tub_cell  = openmc.Cell(name='tub', fill = tuballoy, region = tub_region)\n",
    "\n",
    "geometry = openmc.Geometry([or_cell, tub_cell])\n",
    "geometry.export_to_xml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7464baa9-8c5c-48a2-9153-6c131fee8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_universe = openmc.Universe(cells = (or_cell, tub_cell))\n",
    "model.geometry = openmc.Geometry(root_universe)\n",
    "cell = openmc.Cell(name = 'cell',fill = root_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0a9b262-5653-4d9d-8f2f-cb75ee9883cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Settings \n",
    "settings = openmc.Settings()\n",
    "settings.batches = 3000\n",
    "settings.inactive = 20\n",
    "settings.particles = 10000\n",
    "settings.runmode = 'eigenvalue'\n",
    "\n",
    "#Define Source\n",
    "lower_left = (-1, -1, -1)\n",
    "upper_right = (1, 1, 1)\n",
    "uniform_dist = openmc.stats.Box(lower_left, upper_right)\n",
    "#energy_dist = openmc.stats.Uniform(a=0.0e6,b=20.0e6)\n",
    "settings.source = openmc.IndependentSource(space=uniform_dist)\n",
    "settings.export_to_xml()\n",
    "model.settings = settings\n",
    "\n",
    "\n",
    "settings.output = {'tallies':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2d6904c5-7625-46ec-bb77-de34bedaa02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = './mgxs_' + cell.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a9ebf203-311f-47fc-9ea6-eb51333b4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Energy Groups\n",
    "groups = openmc.mgxs.EnergyGroups(group_edges='CASMO-40')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33530c93-e323-4239-990f-5a9afff100c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jae123p/miniconda3/envs/openmc-env/lib/python3.12/site-packages/openmc/mgxs/library.py:401: RuntimeWarning: The P0 correction will be ignored since the scattering order 3 is greater than zero\n",
      "  warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Initialize MGXS Library\n",
    "mgxs_lib = openmc.mgxs.Library(model.geometry)\n",
    "\n",
    "# Add the chosen group structure\n",
    "mgxs_lib.energy_groups = groups\n",
    "\n",
    "# Scatttering Format and Legendre Order\n",
    "mgxs_lib.scatter_format = \"legendre\"\n",
    "mgxs_lib.legendre_order = 3\n",
    "\n",
    "# Specify multi-group cross-section types to compute\n",
    "mgxs_lib.mgxs_types = ['total']\n",
    "# set uncertainty goal\n",
    "mgxs_lib.tally_trigger = openmc.Trigger('std_dev', 1e-4)\n",
    "\n",
    "# Compute cross sections on a nuclide-by-nuclide basis\n",
    "mgxs_lib.by_nuclide = False\n",
    "\n",
    "# Specify a \"cell\" domain type for the cross section tally filters\n",
    "mgxs_lib.domain_type = 'universe'\n",
    "\n",
    "# Specify the cell domains over which to compute multi-group cross sections\n",
    "#mgxs_lib.domains = model.geometry.get_all_cells().values()\n",
    "\n",
    "# Construct all tallies needed for the multi-group cross section library\n",
    "mgxs_lib.build_library()\n",
    "\n",
    "tallies = openmc.Tallies()\n",
    "mgxs_lib.add_to_tallies_file(tallies, merge=True)\n",
    "model.tallies = tallies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "099a5fb0-19ba-4f84-bf0c-9545a367c809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tallies to Calculate Neutron Flux\n",
    "energies = openmc.mgxs.GROUP_STRUCTURES['XMAS-172']\n",
    "XMAS_filter = openmc.EnergyFilter(openmc.mgxs.GROUP_STRUCTURES['XMAS-172'])\n",
    "\n",
    "tally_XMAS = openmc.Tally()\n",
    "tally_XMAS.filters = [XMAS_filter]\n",
    "tally_XMAS.scores = ['flux']\n",
    "\n",
    "model.tallies.append(tally_XMAS)\n",
    "model.export_to_model_xml(path = my_path + '/' + cell.name + '.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3767dc82-38e7-4984-823e-7db4b845db7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LAPTOP-H8MU6HJU:03915] shmem: mmap: an error occurred while determining whether or not /tmp/ompi.LAPTOP-H8MU6HJU.1000/jf.0/1070923776/shared_mem_cuda_pool.LAPTOP-H8MU6HJU could be created.\n",
      "[LAPTOP-H8MU6HJU:03915] create_and_attach: unable to create shared memory BTL coordinating structure :: size 134217728 \n",
      "                                %%%%%%%%%%%%%%%\n",
      "                           %%%%%%%%%%%%%%%%%%%%%%%%\n",
      "                        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "                      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "                    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "                   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "                                    %%%%%%%%%%%%%%%%%%%%%%%%\n",
      "                                     %%%%%%%%%%%%%%%%%%%%%%%%\n",
      "                 ###############      %%%%%%%%%%%%%%%%%%%%%%%%\n",
      "                ##################     %%%%%%%%%%%%%%%%%%%%%%%\n",
      "                ###################     %%%%%%%%%%%%%%%%%%%%%%%\n",
      "                ####################     %%%%%%%%%%%%%%%%%%%%%%\n",
      "                #####################     %%%%%%%%%%%%%%%%%%%%%\n",
      "                ######################     %%%%%%%%%%%%%%%%%%%%\n",
      "                #######################     %%%%%%%%%%%%%%%%%%\n",
      "                 #######################     %%%%%%%%%%%%%%%%%\n",
      "                 ######################     %%%%%%%%%%%%%%%%%\n",
      "                  ####################     %%%%%%%%%%%%%%%%%\n",
      "                    #################     %%%%%%%%%%%%%%%%%\n",
      "                     ###############     %%%%%%%%%%%%%%%%\n",
      "                       ############     %%%%%%%%%%%%%%%\n",
      "                          ########     %%%%%%%%%%%%%%\n",
      "                                      %%%%%%%%%%%\n",
      "\n",
      "                 | The OpenMC Monte Carlo Code\n",
      "       Copyright | 2011-2024 MIT, UChicago Argonne LLC, and contributors\n",
      "         License | https://docs.openmc.org/en/latest/license.html\n",
      "         Version | 0.15.0\n",
      "        Git SHA1 | 84fb85977f46e162fea8f5a20ea653a3ec62ac24\n",
      "       Date/Time | 2025-05-05 00:09:59\n",
      "   MPI Processes | 1\n",
      "  OpenMP Threads | 24\n",
      "\n",
      " Reading model XML file 'model.xml' ...\n",
      " WARNING: Other XML file input(s) are present. These files may be ignored in\n",
      "          favor of the model.xml file.\n",
      " Reading cross sections XML file...\n",
      " Reading U234 from /home/jae123p/endfb80/endfb-viii.0-hdf5/neutron/U234.h5\n",
      " Reading U235 from /home/jae123p/endfb80/endfb-viii.0-hdf5/neutron/U235.h5\n",
      " Reading U238 from /home/jae123p/endfb80/endfb-viii.0-hdf5/neutron/U238.h5\n",
      " Minimum neutron data temperature: 294 K\n",
      " Maximum neutron data temperature: 294 K\n",
      " Preparing distributed cell instances...\n",
      " Writing summary.h5 file...\n",
      "HDF5-DIAG: Error detected in HDF5 (1.14.3) MPI-process 0:\n",
      "  #000: H5F.c line 660 in H5Fcreate(): unable to synchronously create file\n",
      "    major: File accessibility\n",
      "    minor: Unable to create file\n",
      "  #001: H5F.c line 614 in H5F__create_api_common(): unable to create file\n",
      "    major: File accessibility\n",
      "    minor: Unable to open file\n",
      "  #002: H5VLcallback.c line 3605 in H5VL_file_create(): file create failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Unable to create file\n",
      "  #003: H5VLcallback.c line 3571 in H5VL__file_create(): file create failed\n",
      "    major: Virtual Object Layer\n",
      "    minor: Unable to create file\n",
      "  #004: H5VLnative_file.c line 94 in H5VL__native_file_create(): unable to create file\n",
      "    major: File accessibility\n",
      "    minor: Unable to open file\n",
      "  #005: H5Fint.c line 1910 in H5F_open(): unable to lock the file\n",
      "    major: File accessibility\n",
      "    minor: Unable to lock file\n",
      "  #006: H5FD.c line 2412 in H5FD_lock(): driver lock request failed\n",
      "    major: Virtual File Layer\n",
      "    minor: Unable to lock file\n",
      "  #007: H5FDsec2.c line 941 in H5FD__sec2_lock(): unable to lock file, errno = 11, error message = 'Resource temporarily unavailable'\n",
      "    major: Virtual File Layer\n",
      "    minor: Unable to lock file\n",
      " ERROR: Failed to open HDF5 file with mode 'w': summary.h5\n",
      "--------------------------------------------------------------------------\n",
      "MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD\n",
      "  Proc: [[16341,0],0]\n",
      "  Errorcode: -1\n",
      "\n",
      "NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.\n",
      "You may or may not see output from other processes, depending on\n",
      "exactly when Open MPI kills them.\n",
      "--------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to open HDF5 file with mode 'w': summary.h5 -------------------------------------------------------------------------- MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD Proc: [[16341,0],0] Errorcode: -1 NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes. You may or may not see output from other processes, depending on exactly when Open MPI kills them. --------------------------------------------------------------------------",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sp_file \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmc-env/lib/python3.12/site-packages/openmc/model/model.py:704\u001b[0m, in \u001b[0;36mModel.run\u001b[0;34m(self, particles, threads, geometry_debug, restart_file, tracks, output, cwd, openmc_exec, mpi_args, event_based, export_model_xml, **export_kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexport_to_xml(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexport_kwargs)\n\u001b[1;32m    703\u001b[0m     path_input \u001b[38;5;241m=\u001b[39m export_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 704\u001b[0m     \u001b[43mopenmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeometry_debug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestart_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m               \u001b[49m\u001b[43mtracks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopenmc_exec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmpi_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m               \u001b[49m\u001b[43mevent_based\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;66;03m# Get output directory and return the last statepoint written\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39moutput:\n",
      "File \u001b[0;32m~/miniconda3/envs/openmc-env/lib/python3.12/site-packages/openmc/executor.py:314\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(particles, threads, geometry_debug, restart_file, tracks, output, cwd, openmc_exec, mpi_args, event_based, path_input)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run an OpenMC simulation.\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m \n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m args \u001b[38;5;241m=\u001b[39m _process_CLI_arguments(\n\u001b[1;32m    309\u001b[0m     volume\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, geometry_debug\u001b[38;5;241m=\u001b[39mgeometry_debug, particles\u001b[38;5;241m=\u001b[39mparticles,\n\u001b[1;32m    310\u001b[0m     restart_file\u001b[38;5;241m=\u001b[39mrestart_file, threads\u001b[38;5;241m=\u001b[39mthreads, tracks\u001b[38;5;241m=\u001b[39mtracks,\n\u001b[1;32m    311\u001b[0m     event_based\u001b[38;5;241m=\u001b[39mevent_based, openmc_exec\u001b[38;5;241m=\u001b[39mopenmc_exec, mpi_args\u001b[38;5;241m=\u001b[39mmpi_args,\n\u001b[1;32m    312\u001b[0m     path_input\u001b[38;5;241m=\u001b[39mpath_input)\n\u001b[0;32m--> 314\u001b[0m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/openmc-env/lib/python3.12/site-packages/openmc/executor.py:125\u001b[0m, in \u001b[0;36m_run\u001b[0;34m(args, output, cwd)\u001b[0m\n\u001b[1;32m    122\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpenMC aborted unexpectedly.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    123\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msg\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error_msg)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to open HDF5 file with mode 'w': summary.h5 -------------------------------------------------------------------------- MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD Proc: [[16341,0],0] Errorcode: -1 NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes. You may or may not see output from other processes, depending on exactly when Open MPI kills them. --------------------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "sp_file = model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ef341a7-3ec6-42d6-8fd9-773e1d31580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the last statepoint file\n",
    "sp = openmc.StatePoint(sp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c969f9c-3444-4f0a-9656-62756f6146d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MGXS Library with OpenMC statepoint data\n",
    "mgxs_lib.load_from_statepoint(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2d56a60-e28e-41d6-9efd-902bfb3e2e6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Library' object has no attribute 'export_to_hdf5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Store the cross section data in an \"mgxs/mgxs.h5\" HDF5 binary file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m mgxs_lib\u001b[38;5;241m.\u001b[39mbuild_hdf5_store(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_001.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmgxs_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport_to_hdf5\u001b[49m(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase_002.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Library' object has no attribute 'export_to_hdf5'"
     ]
    }
   ],
   "source": [
    "# Store the cross section data in an \"mgxs/mgxs.h5\" HDF5 binary file\n",
    "mgxs_lib.build_hdf5_store(filename='case_001.h5')\n",
    "mgxs_lib.export_to_hdf5(filename='case_002.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6143ca0f-645d-43c1-815b-a5b357c7e123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
